## 9.1 크롤러와 크롤링

웹 링크를 재귀적으로 따라가는 로봇을 크롤러 혹은 스파이더라고 부른다

인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 크롤러를 사용한다

해당 문서들은 나중에 처리되어 검색 가능한 데이터베이스로 만들어지며, 사용자들이 특정 단어를 포함한 문서를 찾을 수 있게 해준다


### 9.1.1 어디서 시작하는가: ‘루트 집합’

크롤러가 방문을 시작하는 URL들의 초기 집합은 루트 집합(root set)이라고 불린다

일반적으로 웹의 대부분을 커버하기 위해 루트 집합에 너무 많은 페이지가 있을 필요는 없다

좋은 로트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 그리고 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록으로 구성되어 있다


### 9.1.2 링크 추출과 상대 링크 정상화

크롤러는 웹을 돌아다니면서 HTML 문서를 검색하며, 검색한 각 페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 한다

크롤링을 진행하면서 탐색할 때 새 링크를 발견하면서 목록이 급속히 늘어난다


### 9.1.3 순환 피하기

루프나 순환에 빠지지 않도록 매우 조심해야 하기 때문에 그들이 어디를 방문했는지 알아야 한다


### 9.1.4 루프와 중복

순환은 크롤러를 루프에 빠뜨려서 꼼짝 못하게 할 수 있다. 같은 페이지를 반복해서 가져오며 시간을 허비하고, 대역폭을 차지하게 된다

크롤러가 같은 페이지를 반복해서 가져오면 고스란히 웹 서버의 부담이 된다.

크롤러가 많은 수의 중복된 페이지를 가져오게 되는데, 그 결과 쓸모없는 중복 콘텐츠가 넘쳐나게 된다


### 9.1.5 빵 부스러기의 흔적

방문한 곳을 지속적으로 추적하는 것은 쉽지 않다

수십억 개의 URL을 방문해야 하는데, 어떤 URL을 방문하였는지 계속 추적하는 작업은 꽤나 도전적인 일이된다

따라서 속도와 메모리 사용 면에서 효과적이어야 하는 복잡한 자료구조 이다

대규모 웹 크롤러가 그들이 방문한 곳을 관리하기 위해 사용하는 유용한 기법을 몇 가지 들어보면 다음과 같다

**트리와 해시 테이블**
- 복잡한 로봇이라면 방문한 URL을 추적하기 위해 검색 트리나 해시 테이블을 사용했을 수도 있다
- 이들은 URL을 훨씬 빨리 찾아볼 수 있게 해주는 소프트웨어 자료 구조다

**느슨한 존재 비트맵**
- 공간 사용을 최소화하기 위해, 몇몇 대규모 크롤러들은 존재 비트 배열과 같은 느슨한 자료 구조를 사용한다
- 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응되는 ‘존재 비트'를 갖는다
- URL이 크롤링 되었을 때, 해당 존재 비트가 만들어지고, 존재 비트가 이미 있다면 크롤링 된 URL이라고 간주한다

**체크포인트**
- 로봇 프로그램이 갑작스럽게 중단될 경우를 대비해, 방문한 URL의 목록이 디스크에 저장되었는지 확인한다

**파티셔닝**
- 크롤링을 완수하기엔 한 대의 컴퓨터로는 메모리, 디스크 공간, 연산 능력, 네트워크 대역폭이 충분하지 못할 수 있다
- 몇몇 대규모 웹 로봇은 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하고 있는 농장을 이용하며, 각 로봇엔 URL들의 특정 ‘한 부분'이 할당되어 책임을 진다
- 로봇들은 서로 도와 웹을 크롤링 하며 개별 로봇들은 URL들을 이리저리 넘겨주거나, 오동작하는 동료를 도와주거나, 그 외의 이유로 커뮤니케이션을 할 필요가 있다


### 9.1.6 별칭(alias)과 로봇 순환

올바른 자료 구조를 갖추었더라도 URL이 별칭을 가질 수 있는 이상 어떤 페이지를 방문했었는지 말해주는게 쉽지 않을 때도 있다

한 URL이 또 다른 URL에 대한 별칭이라면, 그 둘이 서로 달라보이더라도 사실은 같은 리소스를 가리키고 있다


### 9.1.7 URL 정규화하기

로봇은 다임과 같은 방식으로 모든 URL을 정규화된 형식으로 변환할 수 있다
1.  포트 번호가 명시되지 않았다면, 호스트 명에 ‘:80’을 추가한다
2.  모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다
3.  # 태그들을 제거한다
    
하지만 로봇이 웹 서버에 대한 지식이 필요하며, 한계가 존재한다


### 9.1.8 파일 시스템 링크 순환

파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉터리 계층을 만들 수 있다

때때로 웹 개발자가 로봇을 함정을 빠뜨리기 위해 악의적으로 만들기도 한다


### 9.1.9 동적 가상 웹 공간

같은 서버에 있는 가상 URL에 대한 링크를 포함한 HTML을 즉석에서 만들어 낼 수 있다

가상의 URL로 요청을 받으면 새로운 가상 URL을 갖고 있는 새 HTML 페이지를 날조하여 만들어 낸다


### 9.1.10 루프와 중복 피하기

모든 순환을 피하는 완벽한 방법은 없으며, 휴리스틱의 집합을 필요로 한다

일반적으로 더욱 자율적인 크롤러는 더 쉽게 혼란한 상황에 부딪힌다

휴리스틱은 문제를 피하는데에 도움을 주지만, 약간의 손실을 유발할 수도 있다

웹에서 로봇이 더 올바르게 동작하기 위해 사용되는 기법들은 다음과 같다

**URL 정규화**
- URL을 표준 형태로 변환함으로써 같은 리소스를 가리키는 중복된 URL이 생기는 것을 일부 회피한다

**너비 우선 크롤링**
- 크롤러들은 언제든지 크롤링을 할 수 있는 URL들의 큰 집합을 갖고 있다
- 방문할 URL들을 웹 사이트들 전체에 걸쳐 너비 우선으로 스케쥴링하면, 순환의 영향을 최소화할 수 있다
- 혹시 함정에 들어가더라도, 그 전에 이미 수십만 개의 페이지들을 받아올 수 있다
- 깊이 우선으로 성급하게 들어간다면 순환을 건드리면서 다른 사이트로 빠져나올 수 없게 된다

**스로틀링**
- 로봇이 웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한한다
- 만약 순환을 건드려서 지속적으로 별칭들에 접근을 시도한다면, 스로틀링을 이용해 접근 횟수와 중복의 총 횟수를 제한할 수 있다

**URL 크기 제한**
- 로봇은 일정 길이를 넘는 URL의 크롤링은 거부할 수 있다
- 순환으로 URL이 길어길 경우, 길이 제한으로 순환이 중단된다.
- 반대로 웹 서버들에서는 긴 URL이 주어진 경우 실패하는데 순환되는 로봇이 웹 서버들과 충돌이 되면서 제한된다
- 하지만 해당 기법을 사용하면 가져오지 못하는 콘텐츠들도 틀림없이 있다
- 따라서 URL 길이는 크롤링을 제한하는 방법으로서는 까다롭다
- 하지만 요청 URL이 특정 크기에 도달할 때마다 에러 로그를 남기면서, 감시하는 사용자에게는 훌륭한 신호를 제공한다

**URL/사이트 블랙리스트**
- 로봇 순환을 만들어내거나 함정으로 알려진 사이트나 URL의 목록을 만들어 관리하고 피할 수 있다
- 이는 사람의 손이 필요하며, 악의적인 것이 확실한 사이트들을 피하기 위해사 사용하는 몇 가지 형태의 블랙리스트를 가지고 있다

**패턴 발견**
- 파일 시스템의 심벌릭 링크를 통한 순환과 그와 비슷한 오설정들은 일정 패턴을 따르는 경향이 있다
- 몇몇 로봇은 반복되는 구성요소를 가진 URL을 잠재적인 순환으로 보고, 둘 혹은 셋 이상의 반복된 구성요소를 갖고 있는 URL을 크롤링하는 것을 거절한다

**콘텐츠 지문(fingerprint)**
- 더욱 복잡한 웹 크롤러들 몇몇에 의해 사용되는 중복을 감지하는 직접적인 방법이다
- 로봇들은 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬을 계산한다
- 만약 로봇이 이전에 보았던 체크섬을 가진 페이지를 가져온다면, 크롤링하지 않는다
- 체크섬 함수는 두 페이지가 내용이 달라도 체크섬이 같을 확률은 적어야 하며, MD5와 같은 메시지 요약 함수가 인기 있다
- 어떤 웹 서버들은 동적으로 페이지를 수정하기 때문에, 웹페이지 콘텐츠에 임베딩된 링크와 같은 특정 부분들을 체크섬 계산에서 빠뜨린다

**사람의 모니터링**
- 웹은 거칠며 로봇은 결국 자신에게 적용된 어떤 기법으로도 해결할 수 없는 문제에 봉착한다
- 모든 상용 수준 로봇은 진행 상황을 모니터링 해서 특이한 일이 일어나면 즉각 인지할 수 있도록 진단과 로깅을 포함하여 설계해야한다

---

## 9.2 로봇의 HTTP

로봇들도 HTTP 명세 규칙을 지켜야 하며, 적절한 요청 헤더를 사용해야 한다

하지만 필요한 HTTP를 최소한으로만 구현하려 하기 때문에, 대부분이 요구사항이 적은 HTTP/1.0을 사용한다


### 9.2.1 요청 헤더 식별하기

로봇의 능력, 신원, 출신을 알려주는 몇가지를 알려주기 위해, 대부분은 약간의 신원 식별 헤더를 구현하고 전송한다
- User-Agent: 서버에게 요청을 만든 로봇의 이름
- From: 로봇의 사용자/관리자의 이메일 주소를 제공
- Accept: 서버에게 어떤 미디어 타입을 보내도 되는지 말해주며, 관심있는 유형의 콘텐츠를 받게 해준다
- Referer: 현재의 요청 URL을 포함한 문서의 URL을 제공


### 9.2.2 가상 호스팅

가상 호스팅이 널려 있기에, 요청에 Host 헤더를 포함하지 않으면 로봇이 특정 URL에 대해 잘못된 콘텐츠를 찾게 만든다

만약 Host 헤더를 포함하지 않은 크롤러는, 두 개의 사이트를 운영하는 서버에 요청을 보낼 때 오작동할 수 있다

서버가 기본적으로 A라는 사이트를 보내도록 되어 있으며, Host 헤더를 요구하지 않으면 B로 요청을 보내도 A 사이트 콘텐츠를 받게 된다

게다가 A 콘텐츠를 B 사이트의 콘텐츠로 인식하게 된다


### 9.2.3 조건부 요청

로봇들은 많은 양의 요청을 시도하는데, 검색하는 콘텐츠의 양을 최소화하는 것은 아주 유용하며, 오직 변경되었을 때만 콘텐츠를 가져오도록 하는 것이 좋다

몇몇 로봇은 시간이나 엔터티 태그를 비교하여, 마지막 버전 이후에 업데이트 된 것이 있는지 알아보는 조건부 HTTP 요청을 구현한다

이 방법은 HTTP 캐시의 로컬 사본과 유효성을 검사하는 방법과 유시하다


### 9.2.4 응답 다루기

대다수의 로봇들은 GET 메서드로 콘텐츠를 요청해서 가져오는 것이기 때문에, 응답을 다룰일이 거의 없다

하지만 웹 탐색이나 서버와의 상호작용을 더 잘하려는 로봇들은 여러 종류의 HTTP 응답을 다룰 줄 알아야 한다

**상태 코드**
- 일반적인 상태 코드나 예상할 수 있는 상태 코드를 다룰 수 있어야 한다
- 만약 명시적으로 이해할 수 없는 상태 코드는, 상태 코드가 속한 분류에 근거하여 다루어야 한다

**엔터티**
- HTTP 헤더에 임베딩된 정보에 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있다
- 메타 http-equiv 태그와 메타 태그들은 콘텐츠 저자가 포함시킨 정보로, 해당 정보를 찾아내기 위해 로봇 구현자들은 HEAD 태그를 탐색할 수 있다


### 9.2.5 User-Agent 타게팅

많은 웹 사이트들은 브라우저의 종류를 감지하여 그에 맞게 콘텐츠를 최적화 한다

그를 위해, 사이트 관리자들은 여러 요청을 다루기 위한 전략을 세우며, 로봇이 사이트를 방문했다가 콘텐츠를 얻어갈 수 있어야 한다

----

## 9.3 부적절하게 동작하는 로봇들

**폭주하는 로봇**
- 로봇은 사람보다 훨신 빠르게 HTTP 요청을 만들 수 있으며, 빠른 네트워크 연결을 갖춘 빠른 컴퓨터 위에서 동작한다
- 이 때, 논리적인 에러를 갖고 있거나 순환에 빠졌다면 서버에 극심한 부하를 안겨준다

**오래된 URL**
- 몇몇 로봇은 URL 목록을 방문하며, 만약 웹사이트가 그들의 콘텐츠를 많이 바꾸었다면, 존재하지 않는 URL에 대한 요청을 많이 보낼 수 있다
- 따라서 이에 대한 에러 로그가 채워지거나, 에러 페이지를 제공하는 부하로 인해 서버의 요청 수용능력이 줄어들 수 있다

**길고 잘못된 URL**
- 순환이나 프로그래밍상의 오류로 인해 로봇은 웹 사이트에게 크고 의미없는 URL을 요청할 수 있다
- URL이 너무 길다면, 웹 서버의 처리 능력에 영향을 미치고, 접근 로그를 어지럽게 채우고, 고장을 일으킬수도 있다

**호기심이 지나친 로봇**
- 어떤 로봇들은 사적인 데이터에 대한 URL을 얻어 검색 엔진이나 애플리케이션등을 통해 접근할 수 있도록 만들 수 있다
- 만약 소유자가 알려지기를 원하지 않을 경우, 사생활 침해로 여겨질 수 있다
- 로봇의 구현자들은 사이트 개발자들의 의도치 않은 민감한 데이터를 가져오는 것을 주의해야 한다

**동적 게이트웨이 접근**
- 로봇들은 게이트웨이 애플리케이션의 콘텐츠에 대한 URL로 요청을 할 수 있다
- 이 경우의 데이터는 특수한 목적을 가지고 있으며, 처리 리소스가 많이 들어 사이트 관리자가 좋아하지 않는다

---

## 9.4 로봇 차단하기

사이트 관리자가 로봇의 동작을 제어할 수 있는 기법이 있으며, “Robots Exclusion Standard”라고 이름 지어졌지만, 일반적으로 robots.txt라고 불린다

robots.txt는 서버의 어떤 부분에 접근할 수 있는지에 대한 정보를 주며, 특정 URL을 조회하기 전에 해당 파일을 검사하여야 한다


### 9.4.1 로봇 차단 표준

로봇 차단 표준은 임시방편으로 마련된 표준이며, 접근을 제어하는 능력은 불완전하지만, 없는것보단 훨씬 낫다

차단 표준은 세가지 버전이 존재한다
- 0.0: 로봇 배제 표준 - Disallow 지시자를 지원하는 마틴 코스터의 오리지널 robots.txt 메커니즘
- 1.0: 웹 로봇 제어 방법 - Allow 지시자의 지원이 추가된 마틴 코스터의 IETF 초안
- 2.0: 로봇 차단을 위한 확장 표준 - 정규식과 타이밍 정보를 포함한 숀 코너의 확장. 널리 지원되지는 않는다

오늘날 대부분의 로봇들은 v0.0, v1.0 표준을 택한다


### 9.4.2 웹 사이트와 robots.txt 파일들

웹 사이트의 어떤 URL을 방문하기 전에, robots.txt 파일이 존재하면 로봇은 반드시 파일을 가져와서 처리해야 한다

만약 사이트가 가상 호스팅 된다면, 모든 가상 docroot에 서로다른 robots.txt가 잇을 수 있다

**robots.txt 가져오기**
- 로봇은 HTTP GET 메서드를 통해 robots.txt 리소스를 가져오며, 존재할 경우 text/plain 본문으로 반환하며, 서버가 404를 반환한다면 서버에 로봇 접근을 제한하지 않는것으로 간주한다
- 사이트 관리자가 로봇의 접근을 추적할 수 있도록 From나 User-Agent 헤더를 통해 신원 정볼를 넘기고, 연락처를 제공해야 한다

**응답 코드**
- 로봇은 어떤 웹 사이트든 robots.txt를 찾아보는데 검색 결과에 따라 다르게 동작한다
- 서버가 200을 반환하면 로봇은 응답 콘텐츠를 파하여 차단 규칙을 얻고, 서버에서 무언가를 가져올때마다 해당 규칙을 따른다
- 리소스가 존재하지 않는다고 응답하면, 로봇은 차단 규칙이 존재하지 않는다고 가정하고 제약없이 사이트에 접근한다
- 서버가 접근 제한으로 응답하면, 사이트로의 접근은 완전 제한되어 있다고 가정한다
- 요청 시도가 일시적으로 실패했다면 사이트의 리소스를 검색하는 것은 뒤로 미뤄야 한다
- 서버 응답이 리다이렉션을 읨할 경우, 리소스가 발견될 때까지 리다이렉트를 따라가야 한다


### 9.4.3 robots.txt 파일 포맷

robots.txt 파일은 빈 줄, 주석 줄, 규칙 줄의 세가지 종류가 있다

```
User-Agent: slurp
User-Agnet: webcrawler
Disallow: /private

User-Agent: *
Disallow:
```

레코드는 어떤 로봇이 해당 레코드에 영향을 받는지 지정하는 하나 이사의 User-Agent 줄로 시작하며, 이 로봇들이 접근할 수 있는 URL들을 말해주는 Allow 줄과 Disallow 줄이 온다

**User-Agent 줄**
- 각 로봇의 레코드는 하나 이상의 User-Agent 줄로 시작한다
- 로봇의 이름은 로봇의 HTTP GET 요청 안의 User-Agent 헤더를 통해 보내진다
- 만약 로봇이 자신의 이름에 대응하는 User-Agent 줄을 찾지 못하고 와일드 카드도 없는 경우 대응하는 레코드가 없는 것이다
- 따라서 접근에는 어떤 제한도 없다
- 로봇 이름은 ‘대소문자를 구분하지 않는 부분 문자열’ 규칙을 따르기 때문에 조심해야 한다

**Disallow와 Allow 줄들**
- Disallow와 Allow 줄은 User-Agent 줄들 바로 다음에 오며, 특정 로봇에 대해 어떤 경로가 명시적으로 금지되어 있고 허용되는지 기술한다
- 로봇은 요청하려고 하는 URL을 모든 Disallow, Allow 규칙에 순서대로 맞춰보며 첫번째로 맞는 것이 사용된다
- 만약 맞는 것이 없을 경우 허용으로 간주한다

**Disallow/Allow 접두 매칭 (prefix matching)**
- Disallow나 Allow 규칙이 어떤 경로에 적용되려면, 경로 시작부터 규칙 경로의 길이만큼의 문자열이 규칙 경로와 같아야 한다
- 규칙 경로나 URL 경로의 임의의 ‘이스케이핑된' 문자열들은 비교 전에 원래대로 복원된다 (빗금은 예외이며 그대로 매치되어야 한다)
- 어떤 규칙 경로가 빈 문자열이라면, 모든 URL 경로와 매치된다


### 9.4.4 그 외에 알아둘 점

robots.txt 파일을 파싱할 때의 규칙이 몇 가지 더 있다
- robots.txt 파일은 명세가 발전함에 따라 다른 필드를 포함할 수 있지만, 로봇 자신이 이해할 수 없는 필드는 무시해야 한다
- 하위 호환성을 위해, 여러 줄로 나눠적는 것은 허용되지 않는다
- 주석은 허용되며, 주석 문자(#)로 시작해서, 줄바꿈 문자가 나올 때까지 이어지는 주석 내용으로 이루어진다
- 로봇 차단 표준 버전 0.0은 Allow 줄을 지원하지 않아서, 몇몇 로봇은 오직 0.0 버전의 명세만을 구현하고 Allow 줄을 무시하게 된다 이 때 보수적으로 작동할 경우 허용되는 URL도 탐색하지 않을 수 있다


### 9.4.5 robots.txt의 캐싱과 만료

매 파일 접근마다 로봇이 robots.txt 파일을 새로 가져와야 했다면, 로봇을 덜 효율적으로 만들 뿐더러 웹 서버의 부하도 두 배로 늘어난다

대신 로봇은 주기적으로 robots.txt를 가져와서 결과를 캐시해야 한다

robots.txt 파일의 캐싱을 제어하기 위해 표준 HTTP 캐시 제어 메커니즘이 원 서버, 로봇 양쪽 모두에 의해 사용된다

로봇은 HTTP 응답의 Cache-Control과 Expires 헤더에 주의를 기울여야 한다

로봇 명세 초안은 Cache-Control 지시자가 존재하는 경우 7일간 캐싱하도록 하지만 실무에서 보면 너무 길다

따라서 robots.txt가 없는 상태가 캐싱된 상태에서 robots.txt를 뒤늦게 만들어도 로봇은 접근하게된다


### 9.4.6 로봇 차단 펄 코드

…


### 9.4.7 HTML 로봇 제어 META 태그

robots.txt 파일의 단점 중 하나는 콘텐츠의 작성자 개개인이 아니라 웹 사이트 관리자가 소유한다는 것이다

HTML 페이지 저자는 개별 페이지에 접근하는 것을 직접적으로 제한하는 방법이 있으며, HTML META 태그가 그 방법이다
`<META NAME=”ROBOTS” CONTENT=directive-list>`

**로봇 META 지시자**
- META 지시자에는 몇 가지 종류가 있다
	- NOINDEX: 로봇에게 이 페이지를 처리하지 말고 무시하도록 한다
	- NOFOLLOW: 로봇에게 링크한 페이지를 크롤링하지 말라고 한다
	- INDEX: 로봇에게 이 페이지의 콘텐츠를 인덱싱해도 된다고 말한다
	- FOLLOW: 로봇에게 링크한 페이지를 크롤링해도 된다고 한다
	- NOARCHIVE: 로봇에게 페이지의 캐시를 위한 로컬 사본을 만들면 안된다고 한다
	- ALL: INDEX, FOLLOW와 같다
	- NONE: NOINDEX, NOFOLLOW와 같다
- 로봇 META 태그는 반드시 HEAD 섹션에 나타나야 한다
- 또한 name과 content 값은 대소문자를 구분하지 않는다

**검색엔진 META 태그**
- 다른 META 태그들도 존재한다
	- name=DESCRIPTION, content=텍스트: 웹 페이지의 짧은 요약을 정의할 수 있게 해준다
	- name=KEYWORDS, content=쉼표목록: 키워드 검색을 돕기 위한, 웹페이지를 기술하는 단어들의 목록
	- name=REVISIT-AFTER, content=숫자 days: 로봇이나 검색엔진에게, 쉽게 변경될 예정이기 때문에 해당 날짜가 지난 이후 다시 방문해야 함을 말한다

---

## 9.5 로봇 에티켓

1993년 마틴 코스터는 웹 로봇을 만드는 사람들을 위한 가이드라인 목록을 작성했다
1. 신원 식별
    - 로봇의 신원을 밝혀라
    - 기계의 신원을 밝혀라
    - 연락처를 발겨라
2.  동작
    - 긴장하라
    - 대비하라
    - 감시와 로그
    - 배우고 조정하라
3.  스스로를 제한하라
    - URL을 필터링하라
    - 동적 URL을 필터링하라
    - Accept 관련 헤더로 필터링
    - robots.txt에 따르라
    - 스스로를 억제하라
4.  루프와 중복을 견뎌내기, 그리고 그외의 문제들
    - 모든 응답 코드 다루기
    - URL 정규화하기
    - 적극적으로 순환 피하기
    - 함정을 감시하기
5. 블랙리스트를 관리하라
    - 공간 이해하기
    - 대역폭 이해하기
    - 시간 이해하기
    - 분할 정복
6.  신뢰성
    - 철저하게 테스트하라
    - 체크포인트
    - 실패에 대한 유연성
7.  소통
    - 준비하라
    - 이해하라
    - 즉각 대응하라

---

## 9.6 검색엔진

인터넷 검색엔진은 웹 로봇을 가장 광범위하게 사용하며, 전 세계의 어떤 주제에 대한 문서라도 찾을 수 있게 해준다

많은 웹 사용자들의 시작점으로써 사용자들이 관심있는 정보를 찾을 수 있게 도와주는 유용한 서비스를 제공한다

웹 크롤러들은 마치 먹이를 주듯 검색엔진에게 웹에 존재하는 문서들을 가져다주며, 색인을 생성할 수 있게 해준다


### 9.6.1 넓게 생각하라

검색 엔진은 수십억 개의 웹페이지들을 검색하기 위해 복잡한 크롤러를 사용해야 한다

대규모 크롤러가 자신의 작업을 완료하려면 많은 장비를 사용해서 요청을 병렬로 수행할 수 있어야 한다


### 9.6.2 현대적인 검색엔진의 아키텍처

오늘날 검색엔진들은 ‘풀 텍스트 색인(full-text indexes)’라는 복잡한 로컬 데이터베이스르 생성한다

색인은 웹의 모든 문서에 대한 일종의 카탈로그처럼 동작한다

크롤러들은 웹페이지들을 수집하여 가져온 뒤, 풀 텍스트 색인이 추가한다

사용자들은 웹 검색 게이트웨이를 통해 풀 텍스트 색인에 대한 질의를 보낸다

크롤링 한번에 많은 시간이 걸리는 시간이 상당한데, 웹 페이지들은 매 순간 변화하기 때문에 특정 순간에 대한 스냅샷에 불과하다


### 9.6.3 풀 텍스트 색인

단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 알려줄 수 있는 데이터베이스 이다

색인이 생성된 후에는 검색할 필요가 없게 된다


### 9.6.4 질의 보내기

사용자가 질의를 웹 검색엔진 게이트웨이로 보내는 방법은, HTML 폼을 사용자가 채워넣고 브라우저가 HTTP GET이나 POST 요청을 이용해 게이트웨이로 보내는 식이다

게이트웨이 프로그램은 질의를 추출하고 웹 UI 질의를 풀 텍스트 색인을 검색할 때 사용되는 표현식으로 변환한다


### 9.6.5 검색 결과를 정렬하고 보여주기

질의 결과를 위해 검색 엔진이 색인을 한번 사용했다면, 게이트웨이 애플리케이션은 결과 페이지를 즉석에서 만들어낸다

많은 페이지가 주어진 단어를 사용하기 때문에, 결과에 순위를 매기기 위해 똑똑한 알고리즘을 사용한다

관련이 많은 순서대로 결과 문서에 나타낼 수 있도록 문서들 간의 순서를 알 필요가 있다

이를 관련도 랭킹(relevancy ranking)이라고 불리며, 검색 결과의 목록에 점수를 매기고 정렬하는 과정이다

많은 검색엔진이 웹을 크롤링하면서 수집된 통계 데이터를 사용하여 해당 과정을 더 개선한다

검색엔진에 의해 사용되는 알고리즘, 크롤링 팁, 그 외 각종 기교들은 검색엔진의 비밀이다


### 9.6.6 스푸핑

검색 결과에서 더 높은 순위를 차지하기 위한 웹 관리자의 수단은 줄다리기로 이어진다

수많은 키워드들을 나열한 가짜 페이지를 만들거나, 특정 단어에 대한 가짜 페이지를 생성하는 애플리케이션을 만들어 사용한다